{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tutorial 2: Build First Deep Neurel Network (DNN)\n",
    "\n",
    "接續著[上一回](http://www.ycc.idv.tw/YCNote/post/38)，我們已經有一個單層的Neurel Network，緊接著我們來試著一步一步改造它，讓它成為我們常使用的Deep Neurel Network的形式。\n",
    "\n",
    "本單元程式碼可於[Github](https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py)下載。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加Hidden Layer\n",
    "\n",
    "在上一回當中，我們只有一層Neurel Network，也就是做完一個線性轉換後，就直接使用Softmax Layer來轉換成機率表示方式，這樣的結構並不夠powerful，我們需要把它的結構弄的又窄又深，這樣效果才會好，詳細原因請參考[這一篇的介紹](http://www.ycc.idv.tw/YCNote/post/35)。\n",
    "\n",
    "因此，我們來試著加入一層Hidden Layer，來打造成兩層的Neurel Network，並在兩層之間加入Activation Function，為我的Model增加非線性因子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure(self, features, labels, n_hidden, activation, dropout_ratio=0, train=False):\n",
    "    # build neurel network structure and return their predictions and loss\n",
    "    ### Variable\n",
    "    if (not self.weights) or (not self.biases):\n",
    "        self.weights = {\n",
    "            'fc1': tf.Variable(tf.truncated_normal(shape=(self.n_features, n_hidden))),\n",
    "            'fc2': tf.Variable(tf.truncated_normal(shape=(n_hidden, self.n_labels))),\n",
    "        }\n",
    "        self.biases = {\n",
    "            'fc1': tf.Variable(tf.zeros(shape=(n_hidden))),\n",
    "            'fc2': tf.Variable(tf.zeros(shape=(self.n_labels))),\n",
    "        }\n",
    "    ### Structure\n",
    "    # layer 1\n",
    "    fc1 = self.getDenseLayer(features, self.weights['fc1'],\n",
    "                                self.biases['fc1'], activation=activation)\n",
    "    if train:\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=1-dropout_ratio)\n",
    "\n",
    "    # layer 2\n",
    "    logits = self.getDenseLayer(fc1, self.weights['fc2'], self.biases['fc2'])\n",
    "\n",
    "    y_ = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    return (y_, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先在變數需要有二層的fully-connect參數，注意這些參數的大小，受到Hidden Layer的神經元數目`n_hidden`決定。接下來開始建構整個Neurel Network的結構，`fc1`產生一個fully-connect的結果，並且通過Activation Function再輸出，然後進到下一層，第二層直接使用`fc1`的結果當作新的輸入，再做一次fully-connect，並且讓它通過Softmax Layer來完成最後的Logistic轉換，它的loss一樣的是使用cross-entropy來評估。\n",
    "\n",
    "### Activation Function的選擇\n",
    "\n",
    "剛剛提到的Hidden Layer可以採用不同的Activation Function，我列幾個常使用的Activation Function給大家看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j+3uXalz1k0gQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1162c5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x = np.linspace(-3, 3)\n",
    "    tanh = tf.nn.tanh(x).eval()\n",
    "    sigmoid = tf.nn.sigmoid(x).eval()\n",
    "    relu = tf.nn.relu(x).eval()\n",
    "    \n",
    "    plt.plot(x,tanh, 'g', x,sigmoid, 'b', x,relu, 'r')\n",
    "    plt.legend(('tanh',  'sigmoid',  'relu'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這些Activation Function的使用時機可以簡單這樣說，當我們想要輸出值在+1和-1之間時使用tanh，而當我們想要一個輸出值衡為正時使用sigmoid，它可以將輸出值壓在+1和0之間。tanh比sigmoid多了兩個好處，第一，tanh的梯度變化大於sigmoid，有利於訓練效率，第二，tanh的輸出均值為0，可以避免將前層的梯度偏差帶到下一層。\n",
    "\n",
    "不過，以上的這兩種Activation Function在非常深的網路都會有一個共通問題—梯度消失，仔細看上圖，tanh和sigmoid在極大和極小的地方都會彎成平的，所以每過一次這種Activation Function，訊號就會減小一點，當我們在深網路做Backpropagation時，訊號在過程中不斷的被磨損，到了前面的幾層就已經耗損完畢，此時更新的梯度近乎0，也就是梯度消失，那麼前面的這些層就再也訓練不到了。\n",
    "\n",
    "Relu正可以解決梯度消失的問題，如上圖，在正的部分Relu是線性的，所以多少訊號進來就多少訊號出去，如此一來就不會有耗損的問題，但特別注意，因為tanh和sigmoid會將輸出值限制在一個範圍內，所以有Normalization的味道，但是Relu沒有限制，Normalization可以使我們訓練的效率提升（因為梯度的方向可以直指低點），所以Relu常常會搭配Normalization Layer一起使用，來額外做Normalization，或者是最近一篇Paper提到的一種新的Activation Function：SELU，類似於Relu但是輸出值會是Normalize過的，非常神奇，在這邊我不多論述。\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "在上一回當中，我們的Gradient Descent採用的是將所有的Data一次全考慮進去，評估完所有的Data在一次性的更新權重參數，這樣的作法好處是比較穩定，因為我考慮的是真正的Training Set的E<sub>in</sub>，但缺點就是計算時間長，因為要考慮所有Training Set的每筆數據，需要做大矩陣的計算，而且通常Training Set的數據量也不可以太小，這麼一來計算時間就會拉的很長。\n",
    "\n",
    "因此，有另外一種作法一次只考慮「一筆」數據，使用一筆數據來評估並更新權重參數，每一筆雖然更新結果都不怎麼準確，但是當我隨著時間看過整個Training Set後，就會有平均的效果，所以最後只要Learning Rate不要太大，最後的結果還是可以朝向最佳解的，這個手法會使得Gradient Descent具有隨機性，因此又被稱為Stochastic Gradient Descent，它所帶來的優點是計算時間變短了，我們將可以避免去涉及大矩陣的運算，但缺點是一次只評估一筆數據，將會非常的不穩定。\n",
    "\n",
    "另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Mini-Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，Mini-Batch Gradient Descent一次評估k組數據，並更新參數W，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的架構設計是非常有利於矩陣計算的，因為GPU會利用它強大的平行化將矩陣運算中每個元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Mini-Batch Gradient Descent，這個k值不要超過GPU平行計算所能容納的最大上限，這是個既有效率又更為穩定的作法，順道一提Tensorflow是可以支援GPU的計算的。\n",
    "\n",
    "實務經驗告訴我們Mini-Batch Gradient Descent雖然穩定性比Gradient Descent差，但是收斂的速度卻一點都不輸給Gradient Descent，原因就出在更新的次數，Mini-Batch Gradient Descent一次看的數據筆數比較少，所以Mini-Batch一個Epoch可以更新參數好幾次，而Gradient Descent卻只能更新一次，Mini-Batch的靈敏性，使得它的收斂速度更為快速。打個比方，就好像是兩艘船在搜尋小島，Gradient Descent像是巡洋艦，它有更好設備可以有更好的觀測能力，但是因為它的笨重造成它反應不夠靈敏，Mini-Batch Gradient Descent就像是小船一樣，雖然觀測設備沒這麼好，但是反應靈敏，卻是可以更容易率先找到小島。\n",
    "\n",
    "那我們來看看要怎麼做到Mini-Batch Gradient Descent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, epochs=10, validation_data=None, test_data=None, batch_size=None):\n",
    "    ...略...\n",
    "\n",
    "    self.sess.run(self.init_op)\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %2d/%2d: ' % (epoch+1, epochs))\n",
    "\n",
    "        # mini-batch gradient descent\n",
    "        index = [i for i in range(N)]\n",
    "        random.shuffle(index)\n",
    "        while len(index) > 0:\n",
    "            index_size = len(index)\n",
    "            batch_index = [index.pop() for _ in range(min(batch_size, index_size))]\n",
    "\n",
    "            feed_dict = {\n",
    "                self.train_features: X[batch_index, :],\n",
    "                self.train_labels: y[batch_index],\n",
    "            }\n",
    "            _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "\n",
    "            print('[%d/%d] loss = %9.4f     ' % (N-len(index), N, loss), end='\\r')\n",
    "\n",
    "        ...略..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每一個`epoch`都完整的看過數據一遍，而mini-batch gradient descent是隨機取`batch_size`筆數據來更新權重，所以我採用這樣的作法，先依照數據的筆數`N`列出可能的Index有哪些，然後再做一個`random.shuffle`來做到隨機採樣，然後接下來只要簡單的從前面取`batch_size`筆數據進行更新，直到用盡所有的index為止，就可以做到mini-batch的效果。\n",
    "\n",
    "### Regularization\n",
    "\n",
    "當你開始加深你的DNN時，就已經在增加Model的複雜度，增加複雜度想當然爾的可以增加對於數據的描述能力，在分類問題中代表可以增加精確度，不過要特別注意Overfitting的出現，當Model越複雜越容易產生Overfitting，Overfitting的結果是有看過的數據描述的很好，但沒看過的數據預測就很差，所以在Training的過程要特別注意Validation Set的表現，如果發現Training Set的表現越來越好，但是Validation Set的表現裹足不前甚至變的更差，那就很有可能已經Overfitting了。\n",
    "\n",
    "如果你已經看到Overfitting出現了，有什麼方法可以抑制他呢？這個時候就需要Regularization的幫忙，在Neurel Network常見的Regularization有兩種：Weight Regularization和Dropout，待會會一一介紹。\n",
    "\n",
    "這邊特別注意，不要每次精確度沒有提升就怪Overfitting！如果連你的Training Set都沒辦法有好的表現，這就可能不是Overfitting，反而可能是Underfitting，這個時候不要再增加Regularization，而是試著去調整Learning Rate，或者增加模型的複雜度，加深DNN或增加神經元的數目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Regularization\n",
    "\n",
    "我們可以藉著在Loss Function裡頭加入Weight的貢獻，來達到限制W的大小的目標，這樣做可以降低Overfitting，詳細原理請[參考這篇的Regularization的部分](http://www.ycc.idv.tw/YCNote/post/28)。\n",
    "\n",
    "我們來看看應該怎麼做到L2 Regularization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization loss\n",
    "regularization = tf.reduce_mean([tf.nn.l2_loss(w) / tf.size(w,out_type=tf.float32) for w in self.weights.values()])\n",
    "# total loss\n",
    "loss = original_loss + alpha * regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡我習慣將Loss對Weights的個數做平均，這有一個好處當我在調整神經元數目時，`alpha`可以不需要大動作調整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout是Deep Learning常用的Regularization技巧，它的作法是在訓練的時候我先隨機把部份的神經元關閉，使用較少的神經元訓練，來達到Regularization的效果，最後在「推論」的時候再使用全部的神經元，我個人覺得這有Aggregation Model的味道，分別訓練出許多的sub-model再做Aggregation以達到截長補短的效果。\n",
    "\n",
    "實作上有一些細節必須要注意，當我們關閉一些神經元時，也就是等於減少部份的貢獻量，所以我們需要依照相應比例來給予權重，以抵銷減少的部分。舉個例子，假設今天原本應該要輸出的值有十個，這十個值都是1，然後因為Dropout，變成五個1五個0的輸出，我們看到原本貢獻量因為Dropout一半而少一半，這樣並不合理，會導致我後面的Weights在更新的時候低估更新量，所以我們必須要將「沒被Dropout的部分」權重乘上一倍，才可以解決問題。因此，如果Dropout r倍的神經元，權重就要乘以(1/r)倍，我們來看看Tensorflow怎麼做的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original S =\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.]\n",
      " [ 5.  5.  5.  5.  5.  5.  5.  5.]]\n",
      "Dropout S =\n",
      "[[  2.   2.   2.   0.   0.   2.   0.   0.]\n",
      " [  6.   0.   6.   0.   6.   6.   0.   0.]\n",
      " [ 10.  10.  10.  10.  10.   0.   0.  10.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    S = tf.constant([[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                           [3, 3, 3, 3, 3, 3, 3, 3],\n",
    "                           [5, 5, 5, 5, 5, 5, 5, 5]],tf.float32)  # 3 Data, 8 dim. Score\n",
    "    print('Original S =')\n",
    "    print(S.eval())\n",
    "    S_drop = tf.nn.dropout(S,keep_prob=0.5)  # dropout ratio = 1 - keep_prob = 0.5\n",
    "    print('Dropout S =')\n",
    "    print(S_drop.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們看到因為dropout 0.5倍，所以輸出值權重乘上2倍，另外一提，Tensorflow的Dropout機制是隨機的，所以Drop out的比例會接近我們想要的比例，但不是絕對剛好。\n",
    "\n",
    "那我要怎麼把Dropout放進去我的Model呢？特別注意，我們並不希望已經Training完的Model還有Dropout這一層，所以我在`structure`裡頭設計一個`train`的開關，當我在Training過程就把它打開，Dropout這一層就會被加進去，「推論」的時候就關閉，保持原有的神經元數量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def structure(self,features,labels,n_hidden,activation,dropout_ratio=0,train=False):\n",
    "        \n",
    "        ... 略 ...\n",
    "        \n",
    "        # layer 1\n",
    "        fc1 = self.getDenseLayer(features,self.weights['fc1'], self.biases['fc1'], activation=activation)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1,keep_prob=1-dropout_ratio)\n",
    "            \n",
    "        # layer 2\n",
    "        logits = self.getDenseLayer(fc1,self.weights['fc2'],self.biases['fc2'])\n",
    "        \n",
    "        ... 略 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer的選擇\n",
    "\n",
    "我們可以自由的替換我們想要使用的Optimizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training operation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "self.train_op = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensorflow中目前有以下十種Optimizer供我們使用。\n",
    "\n",
    "* tf.train.GradientDescentOptimizer\n",
    "* tf.train.AdadeltaOptimizer\n",
    "* tf.train.AdagradOptimizer\n",
    "* tf.train.AdagradDAOptimizer\n",
    "* tf.train.MomentumOptimizer\n",
    "* tf.train.AdamOptimizer\n",
    "* tf.train.FtrlOptimizer\n",
    "* tf.train.ProximalGradientDescentOptimizer\n",
    "* tf.train.ProximalAdagradOptimizer\n",
    "* tf.train.RMSPropOptimizer\n",
    "\n",
    "如果想要了解每個Optimizer的演算法可以參考[這篇有詳細的說明](http://ruder.io/optimizing-gradient-descent/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 來看看程式怎麼寫\n",
    "\n",
    "講了那麼多，來看看完整的程式怎麼寫？照慣例，先畫個流程圖。\n",
    "\n",
    "![DNNLogisticClassification](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.003.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNLogisticClassification:\n",
    "\n",
    "    def __init__(self, n_features, n_labels,\n",
    "                 learning_rate=0.5, n_hidden=1000, activation=tf.nn.relu,\n",
    "                 dropout_ratio=0.5, alpha=0.0):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "        self.graph = tf.Graph()  # initialize new graph\n",
    "        self.build(learning_rate, n_hidden, activation,\n",
    "                   dropout_ratio, alpha)  # building graph\n",
    "        self.sess = tf.Session(graph=self.graph)  # create session by the graph\n",
    "\n",
    "    def build(self, learning_rate, n_hidden, activation, dropout_ratio, alpha):\n",
    "        # Building Graph\n",
    "        with self.graph.as_default():\n",
    "            ### Input\n",
    "            self.train_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "\n",
    "            ### Optimalization\n",
    "            # build neurel network structure and get their predictions and loss\n",
    "            self.y_, self.original_loss = self.structure(features=self.train_features,\n",
    "                                                         labels=self.train_labels,\n",
    "                                                         n_hidden=n_hidden,\n",
    "                                                         activation=activation,\n",
    "                                                         dropout_ratio=dropout_ratio,\n",
    "                                                         train=True)\n",
    "            # regularization loss\n",
    "            self.regularization = \\\n",
    "                tf.reduce_sum([tf.nn.l2_loss(w) for w in self.weights.values()]) \\\n",
    "                / tf.reduce_sum([tf.size(w, out_type=tf.float32) for w in self.weights.values()])\n",
    "\n",
    "            # total loss\n",
    "            self.loss = self.original_loss + alpha * self.regularization\n",
    "\n",
    "            # define training operation\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "            ### Prediction\n",
    "            self.new_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.new_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "            self.new_y_, self.new_original_loss = self.structure(features=self.new_features,\n",
    "                                                                 labels=self.new_labels,\n",
    "                                                                 n_hidden=n_hidden,\n",
    "                                                                 activation=activation)\n",
    "            self.new_loss = self.new_original_loss + alpha * self.regularization\n",
    "\n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def structure(self, features, labels, n_hidden, activation, dropout_ratio=0, train=False):\n",
    "        # build neurel network structure and return their predictions and loss\n",
    "        ### Variable\n",
    "        if (not self.weights) or (not self.biases):\n",
    "            self.weights = {\n",
    "                'fc1': tf.Variable(tf.truncated_normal(shape=(self.n_features, n_hidden))),\n",
    "                'fc2': tf.Variable(tf.truncated_normal(shape=(n_hidden, self.n_labels))),\n",
    "            }\n",
    "            self.biases = {\n",
    "                'fc1': tf.Variable(tf.zeros(shape=(n_hidden))),\n",
    "                'fc2': tf.Variable(tf.zeros(shape=(self.n_labels))),\n",
    "            }\n",
    "        ### Structure\n",
    "        # layer 1\n",
    "        fc1 = self.getDenseLayer(features, self.weights['fc1'],\n",
    "                                 self.biases['fc1'], activation=activation)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, keep_prob=1-dropout_ratio)\n",
    "\n",
    "        # layer 2\n",
    "        logits = self.getDenseLayer(fc1, self.weights['fc2'], self.biases['fc2'])\n",
    "\n",
    "        y_ = tf.nn.softmax(logits)\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "                 tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "        return (y_, loss)\n",
    "\n",
    "    def getDenseLayer(self, input_layer, weight, bias, activation=None):\n",
    "        # fully connected layer\n",
    "        x = tf.add(tf.matmul(input_layer, weight), bias)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y, epochs=10, validation_data=None, test_data=None, batch_size=None):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "\n",
    "        N = X.shape[0]\n",
    "        random.seed(9000)\n",
    "        if not batch_size:\n",
    "            batch_size = N\n",
    "\n",
    "        self.sess.run(self.init_op)\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %2d/%2d: ' % (epoch+1, epochs))\n",
    "\n",
    "            # mini-batch gradient descent\n",
    "            index = [i for i in range(N)]\n",
    "            random.shuffle(index)\n",
    "            while len(index) > 0:\n",
    "                index_size = len(index)\n",
    "                batch_index = [index.pop() for _ in range(min(batch_size, index_size))]\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.train_features: X[batch_index, :],\n",
    "                    self.train_labels: y[batch_index],\n",
    "                }\n",
    "                _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "\n",
    "                print('[%d/%d] loss = %9.4f     ' % (N-len(index), N, loss), end='\\r')\n",
    "\n",
    "            # evaluate at the end of this epoch\n",
    "            y_ = self.predict(X)\n",
    "            train_loss = self.evaluate(X, y)\n",
    "            train_acc = self.accuracy(y_, y)\n",
    "            msg = '[%d/%d] loss = %8.4f, acc = %3.2f%%' % (N, N, train_loss, train_acc*100)\n",
    "\n",
    "            if validation_data:\n",
    "                val_loss = self.evaluate(validation_data[0], validation_data[1])\n",
    "                val_acc = self.accuracy(self.predict(validation_data[0]), validation_data[1])\n",
    "                msg += ', val_loss = %8.4f, val_acc = %3.2f%%' % (val_loss, val_acc*100)\n",
    "\n",
    "            print(msg)\n",
    "\n",
    "        if test_data:\n",
    "            test_acc = self.accuracy(self.predict(test_data[0]), test_data[1])\n",
    "            print('test_acc = %3.2f%%' % (test_acc*100))\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return (np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/predictions.shape[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._check_array(X)\n",
    "        return self.sess.run(self.new_y_, feed_dict={self.new_features: X})\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "        return self.sess.run(self.new_loss, feed_dict={self.new_features: X,\n",
    "                                                       self.new_labels: y})\n",
    "\n",
    "    def _check_array(self, ndarray):\n",
    "        ndarray = np.array(ndarray)\n",
    "        if len(ndarray.shape) == 1:\n",
    "            ndarray = np.reshape(ndarray, (1, ndarray.shape[0]))\n",
    "        return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "train_data = mnist.train\n",
    "valid_data = mnist.validation\n",
    "test_data = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/ 3: \n",
      "[55000/55000] loss =   0.4686, acc = 88.78%, val_loss =   0.5565, val_acc = 88.94%\n",
      "Epoch  2/ 3: \n",
      "[55000/55000] loss =   0.3258, acc = 91.82%, val_loss =   0.4364, val_acc = 91.34%\n",
      "Epoch  3/ 3: \n",
      "[55000/55000] loss =   0.2675, acc = 93.05%, val_loss =   0.3570, val_acc = 92.56%\n",
      "test_acc = 92.04%\n"
     ]
    }
   ],
   "source": [
    "model = DNNLogisticClassification(\n",
    "    n_features=28*28,\n",
    "    n_labels=10,\n",
    "    learning_rate=0.5,\n",
    "    n_hidden=1000,\n",
    "    activation=tf.nn.relu,\n",
    "    dropout_ratio=0.5,\n",
    "    alpha=0.01,\n",
    ")\n",
    "model.fit(\n",
    "    X=train_data.images,\n",
    "    y=train_data.labels,\n",
    "    epochs=3,\n",
    "    validation_data=(valid_data.images, valid_data.labels),\n",
    "    test_data=(test_data.images, test_data.labels),\n",
    "    batch_size = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上次的結果比，你會發現有長足的進步，精確率來到90幾，大家可以[下載程式碼](https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py)，試著調整參數使得DNN Model的精確率可以更高，參數包含：\n",
    "* Hidden Layer的神經元數量\n",
    "* 不同的Activation Function\n",
    "* 不同的Batch Size\n",
    "* 調整Weight Regularization的比例\n",
    "* 調整Dropout Ratio\n",
    "* 選擇不同Optimizer\n",
    "* 使得DNN更深\n",
    "\n",
    "調整Model是重要的工作，試著自己動手做做看，你可以讓你的Model有多準呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
