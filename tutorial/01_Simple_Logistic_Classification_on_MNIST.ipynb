{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSLUyGtUfzXa"
   },
   "source": [
    "## Tensorflow Tutorial 1: Simple Logistic Classification on MNIST\n",
    "\n",
    "初次學習Tensorflow最困難的地方莫過於不知道從何下手，已經學會很多的Deep Learning理論，但是要自己使用Tensorflow將Network建起來卻是非常困難的，這篇文章我會先簡單的介紹幾個Tensorflow的概念，最後利用這些概念建立一個簡單的分類模型。\n",
    "\n",
    "本單元程式碼可於[Github]( https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/01_simple_logistic_classification_on_MNIST.py)下載。\n",
    "\n",
    "首先，先`import`一些會用到的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb4jih0XfzXc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8__0TFoIfzXh"
   },
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "定義`summary` function以便於觀察ndarray。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gV2LmkHfzXj"
   },
   "outputs": [],
   "source": [
    "def summary(ndarr):\n",
    "    print(ndarr)\n",
    "    print('* shape: {}'.format(ndarr.shape))\n",
    "    print('* min: {}'.format(np.min(ndarr)))\n",
    "    print('* max: {}'.format(np.max(ndarr)))\n",
    "    print('* avg: {}'.format(np.mean(ndarr)))\n",
    "    print('* std: {}'.format(np.std(ndarr)))\n",
    "    print('* unique: {}'.format(np.unique(ndarr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xe2plTabfzXn"
   },
   "source": [
    "ndarray是numpy的基本元素，它非常便於我們做矩陣的運算。\n",
    "\n",
    "我們使用MNIST Dataset來當作我們練習的標的，MNIST包含一包手寫數字的圖片，每張圖片大小為28x28，每一張圖片都是一個手寫的阿拉伯數字包含0到9，並且標記上它所對應的數字。我們的目標就是要利用MNIST做到手寫數字辨識。\n",
    "\n",
    "在Tensorflow你可以很簡單的得到「處理過後的」MNIST，只要利用以下程式碼，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "geqB9zFafzXp",
    "outputId": "32cdc790-f1f9-4a41-e4f3-520d1d0c7ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "train_data = mnist.train\n",
    "valid_data = mnist.validation\n",
    "test_data = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBRy-koufzXv"
   },
   "source": [
    "每個`train_data`、`valid_data`、`test_data`都包含兩部分：圖片和標籤。\n",
    "\n",
    "我們來看一下圖片的部分，`train_data.images`一共有55000張圖，每一張圖原本大小是28x28，不過特別注意這裡的Data已經先做過預先處理了，因此圖片已經被打平成28x28=784的一維矩陣了，另外每個Pixel的值也先做過「Normalization」了，通常會這樣處理，每個值減去128再除以128，所以你可以從以下的`summary`中看到它的最大最小值落在0到1之間，還有這個Dataset也已經做過亂數重排了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "colab_type": "code",
    "id": "zHqPB3cPfzXw",
    "outputId": "0fe76502-f755-4823-cdfc-9639a3d4bc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "* shape: (55000, 784)\n",
      "* min: 0.0\n",
      "* max: 1.0\n",
      "* avg: 0.13070042431354523\n",
      "* std: 0.30815958976745605\n",
      "* unique: [0.         0.00392157 0.00784314 0.01176471 0.01568628 0.01960784\n",
      " 0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313726\n",
      " 0.04705883 0.0509804  0.05490196 0.05882353 0.0627451  0.06666667\n",
      " 0.07058824 0.07450981 0.07843138 0.08235294 0.08627451 0.09019608\n",
      " 0.09411766 0.09803922 0.10196079 0.10588236 0.10980393 0.1137255\n",
      " 0.11764707 0.12156864 0.1254902  0.12941177 0.13333334 0.13725491\n",
      " 0.14117648 0.14509805 0.14901961 0.15294118 0.15686275 0.16078432\n",
      " 0.16470589 0.16862746 0.17254902 0.1764706  0.18039216 0.18431373\n",
      " 0.18823531 0.19215688 0.19607845 0.20000002 0.20392159 0.20784315\n",
      " 0.21176472 0.21568629 0.21960786 0.22352943 0.227451   0.23137257\n",
      " 0.23529413 0.2392157  0.24313727 0.24705884 0.2509804  0.25490198\n",
      " 0.25882354 0.2627451  0.26666668 0.27058825 0.27450982 0.2784314\n",
      " 0.28235295 0.28627452 0.2901961  0.29411766 0.29803923 0.3019608\n",
      " 0.30588236 0.30980393 0.3137255  0.31764707 0.32156864 0.3254902\n",
      " 0.32941177 0.33333334 0.3372549  0.34117648 0.34509805 0.34901962\n",
      " 0.3529412  0.35686275 0.36078432 0.3647059  0.36862746 0.37254903\n",
      " 0.37647063 0.3803922  0.38431376 0.38823533 0.3921569  0.39607847\n",
      " 0.40000004 0.4039216  0.40784317 0.41176474 0.4156863  0.41960788\n",
      " 0.42352945 0.427451   0.43137258 0.43529415 0.43921572 0.4431373\n",
      " 0.44705886 0.45098042 0.454902   0.45882356 0.46274513 0.4666667\n",
      " 0.47058827 0.47450984 0.4784314  0.48235297 0.48627454 0.4901961\n",
      " 0.49411768 0.49803925 0.5019608  0.5058824  0.50980395 0.5137255\n",
      " 0.5176471  0.52156866 0.5254902  0.5294118  0.53333336 0.5372549\n",
      " 0.5411765  0.54509807 0.54901963 0.5529412  0.5568628  0.56078434\n",
      " 0.5647059  0.5686275  0.57254905 0.5764706  0.5803922  0.58431375\n",
      " 0.5882353  0.5921569  0.59607846 0.6        0.6039216  0.60784316\n",
      " 0.6117647  0.6156863  0.61960787 0.62352943 0.627451   0.6313726\n",
      " 0.63529414 0.6392157  0.6431373  0.64705884 0.6509804  0.654902\n",
      " 0.65882355 0.6627451  0.6666667  0.67058825 0.6745098  0.6784314\n",
      " 0.68235296 0.6862745  0.6901961  0.69411767 0.69803923 0.7019608\n",
      " 0.7058824  0.70980394 0.7137255  0.7176471  0.72156864 0.7254902\n",
      " 0.7294118  0.73333335 0.7372549  0.7411765  0.74509805 0.7490196\n",
      " 0.75294125 0.7568628  0.7607844  0.76470596 0.7686275  0.7725491\n",
      " 0.77647066 0.7803922  0.7843138  0.78823537 0.79215693 0.7960785\n",
      " 0.8000001  0.80392164 0.8078432  0.8117648  0.81568635 0.8196079\n",
      " 0.8235295  0.82745105 0.8313726  0.8352942  0.83921576 0.8431373\n",
      " 0.8470589  0.85098046 0.854902   0.8588236  0.86274517 0.86666673\n",
      " 0.8705883  0.8745099  0.87843144 0.882353   0.8862746  0.89019614\n",
      " 0.8941177  0.8980393  0.90196085 0.9058824  0.909804   0.91372555\n",
      " 0.9176471  0.9215687  0.92549026 0.9294118  0.9333334  0.93725497\n",
      " 0.94117653 0.9450981  0.9490197  0.95294124 0.9568628  0.9607844\n",
      " 0.96470594 0.9686275  0.9725491  0.97647065 0.9803922  0.9843138\n",
      " 0.98823535 0.9921569  0.9960785  1.        ]\n"
     ]
    }
   ],
   "source": [
    "summary(train_data.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wT5YM4oGfzX4"
   },
   "source": [
    "來試著畫圖來看看，我們使用ndarray的index功能來選出第10張圖片，`train_data.images[10,:]`表示的是選第一軸的第10個和第二軸的全部。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6R6j8WcafzX5"
   },
   "outputs": [],
   "source": [
    "def plot_fatten_img(ndarr):\n",
    "    img = ndarr.copy()\n",
    "    img.shape = (28,28)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "1iZMitxzfzX9",
    "outputId": "5d747a32-21ff-4a1e-d225-2bef7b5a1df1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8ERfiBoAg/EBThB4Ii/EBQhB8I6v8A+Md7QMI5IyUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_fatten_img(train_data.images[10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xTkbTtEfzYA"
   },
   "source": [
    "很顯而易見的，這是一個0。\n",
    "\n",
    "接下來來看標籤的部分，`train_data.labels`不意外的一樣的也是有相應的55000筆資料，所對應的就是前面的每一張圖片，總共有10種類型:0到9，所以大小為(55000, 10)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "7T3xY8FefzYB",
    "outputId": "e98ee934-c542-4eec-cb56-2abfb1eba2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "* shape: (55000, 10)\n",
      "* min: 0.0\n",
      "* max: 1.0\n",
      "* avg: 0.1\n",
      "* std: 0.30000000000000004\n",
      "* unique: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "summary(train_data.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA4oSl6rfzYG"
   },
   "source": [
    "所以我們來看看上面那張圖片的標籤，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "fjQG0pT3fzYH",
    "outputId": "d178d1e5-8efa-4c27-b6e7-bbc59f83e366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqoZq-xofzYM"
   },
   "source": [
    "看起來的確沒錯，在0的位置標示1.，而其他地方標示為0.，因此這是一個標示為0的label沒有錯，這種表示方法稱為One-Hot Encoding，它具有機率的涵義，所代表的是有100%的機會落在0的類別上。\n",
    "\n",
    "### Softmax\n",
    "\n",
    "通常One-Hot Encoding會搭配Softmax一同服用，最後的Output結果如果是機率分布，那我也需要讓我的Neurel Network可以輸出機率分布。\n",
    "\n",
    "![softmax](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.001.jpeg)\n",
    "\n",
    "通過Softmax這一層，我們就可以將輸出轉變為以「機率」表示。\n",
    "\n",
    "我們可以來手刻一個Softmax Function，不過直接套用Tensorflow中函數的也是可以的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "un0KnUEGfzYN",
    "outputId": "32e074d3-7312-43d6-abe0-35a1a6e33281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    # avoid exp function go to too large,\n",
    "    # pre-reduce before applying exp function\n",
    "    max_score = np.max(x, axis=0)\n",
    "    x = x - max_score\n",
    "    \n",
    "    exp_s = np.exp(x)\n",
    "    sum_exp_s = np.sum(exp_s, axis=0)\n",
    "    softmax = exp_s / sum_exp_s\n",
    "    return softmax\n",
    "\n",
    "scores = [3.0, 1.0, 0.2]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb65iA9kfzYP"
   },
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "一旦我們要處理機率預測的問題，就不可以使用單純的「平方誤差」，而必須使用Cross-Entropy Loss，是這樣計算的：\n",
    "\n",
    "$$\n",
    "Loss_{cross-entropy} = - \\sum_i y_i ln(s_i)\n",
    "$$\n",
    "其中，$y_i$為目標Label，$s_i$為經過Softmax產生的預測值。\n",
    "\n",
    "至於如果你想要了解為何需要使用Cross-Entropy Loss？這我在機器學習基石的筆記中已經有提及過，請看[介紹Logistic Regression的部分](http://www.ycc.idv.tw/YCNote/post/27)。\n",
    "\n",
    "### 分離數據的重要性\n",
    "\n",
    "在MNIST Dataset中，你會發現分為Training Dataset、Validation Dataset和Testing Dataset，這樣的作法在Machine Learning中是常見且必要的。\n",
    "\n",
    "流程是這樣的，我們會先使用Training Dataset來訓練Model，並且使用Validation Dataset來檢驗Model的好壞，我們會依據Validation Dataset的檢驗調整Model上的參數，試著盡可能的壓低Validation Dataset的Error，記住！在過程中所產生的所有Models都要保留下來，因為最後選擇的Model並不是Validation Dataset的Error最小的，而是要再由Testing Dataset來做最後的挑選，挑選出能使Testing Dataset的Error最小的Model。\n",
    "\n",
    "這所有的作法都是為了避免Overfitting的情況發生，也就是機器可能因為看過一筆Data，結果就把這筆Data給完整記了起來，而Data本身含有雜訊，雜訊就這樣滲透到Model裡，確實做到分離是很重要的，讓Model在測試階段時可以使用沒有看過的Data。\n",
    "\n",
    "因此，Validation Dataset的分離是為了避免讓Model在Training階段看到要驗證的資料，所以更能正確的評估Model的好壞。但這樣是不夠的，人為會根據Validation Dataset來調整Model，這樣無形之中已經將Validation Dataset的資訊間接的經由人傳給了Model，所以還是沒有徹底分離，因此在最後挑選Models時，我們會使用另外一筆從沒看過的資料Testing Dataset來做挑選，一旦挑選完就不能再去調整任何參數了。\n",
    "\n",
    "\n",
    "### Tensorflow工作流程\n",
    "\n",
    "我們這一篇將會使用Tensorflow實作最簡單的單層Neurel Network，在這之前我們來看看Tensorflow是如何運作的？\n",
    "\n",
    "深度學習是由一層一層可以微分的神經元所連接而成，數學上可以表示為張量(Tensor)的表示式，我們一般講的矩陣運算是指2x2的矩陣運算，而張量(Tensor)則是拓寬到n維陣列做計算，在Machine Learning當中我們常常需要處理到相當高維度的計算，例如：有五張28x28的彩色圖的表示就必須使用到四維張量，第一維表示第幾張、第二、三維表示圖片的大小、第四維則表示RGB，如果你是物理系的學生應該也對張量不陌生，廣義相對論裡頭大量的使用四維張量運算，三維空間加一維時間。\n",
    "\n",
    "而在做Neurel Network時，我們會根據需求不同設計不同形式但合理的流程(Flow)，再使用數據來訓練我的Model。所以，這就是Tensorflow命名由來：Tensor+Flow。\n",
    "\n",
    "因此，一開始要先設計Model的結構，這在Tensorflow裡頭稱為Graph，Graph的作用是事先決定Neurel Network的結構，決定Neuron要怎麼連接？決定哪一些窗口是可以由外部置放數據的？決定哪一些變數是可以被訓練的？哪一些變數是不可以被訓練的？定義將要怎麼樣優化這個系統？...等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2Q9E5pifzYP"
   },
   "outputs": [],
   "source": [
    "my_graph = tf.Graph() # Initialize a new graph\n",
    "\n",
    "with my_graph.as_default(): # Create a scope to build graph\n",
    "    # ...\n",
    "    # detail of building graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwigLuGvfzYT"
   },
   "source": [
    "Graph只是一個結構，它不具有有效的資訊，而當我們定義完成Graph之後，接下來我們需要創造一個環境叫做Session，Session會將Graph的結構複製一份，然後再放入資訊進行Training或是預測等等，因此Session是具有有效資訊的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_CrKKyNfzYU"
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=my_graph) as sess: # Copy graph into session\n",
    "    # ...\n",
    "    # detail of doing machine learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNCDM4W7fzYW"
   },
   "source": [
    "還有另外一種寫法也是相同作用的，我個人比較喜歡下面這種寫法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw55D3iSfzYW"
   },
   "outputs": [],
   "source": [
    "my_session = tf.Session(graph=my_graph)\n",
    "my_session.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ye469pWkfzYb"
   },
   "source": [
    "### Tensorflow的基本「張量」元素\n",
    "\n",
    "接下來我們就來看看有哪些構成Graph的基本元素可以使用。\n",
    "\n",
    "(1) 常數張量：\n",
    "\n",
    "一開始來看看「常數張量」，常數指的是在Model中不會改變的數值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F37wqyyjfzYc"
   },
   "outputs": [],
   "source": [
    "tensor = tf.constant([1, 2, 3, 4, 5, 6, 7], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YeRPOIwfzYd"
   },
   "source": [
    "(2) 變數張量：\n",
    "\n",
    "與常數截然不同的就是變數，「變數張量」是指在訓練當中可以改變的值，一般「變數張量」會用作於Machine Learning需要被訓練的參數，如果你沒有特別設定，在最佳化的過程中，Tensorflow會自動調整「變數張量」的數值來最佳化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "4dtZwlVrfzYe",
    "outputId": "6123477a-304d-46bf-f366-0c21b81b7665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.Variable(tf.truncated_normal(shape=(3, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5u0rzpLfzYf"
   },
   "source": [
    "因為變數通常是未知且待優化的參數，所以我們一般會使用Initalizer來設定它的初始值，`tf.truncated_normal(shape=(3,5))`會隨機產生大小3x5的矩陣，它的值呈常態分佈但只取兩個標準差以內的數值。\n",
    "\n",
    "如果今天你想要有一個「變數張量」但是又不希望它因為最佳化而改變，這時你要特別指定`trainable`為`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmr1KLJafzYf"
   },
   "outputs": [],
   "source": [
    "tensor = tf.Variable(5, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft68RaznfzYg"
   },
   "source": [
    "(3) 置放張量：\n",
    "\n",
    "另外有一些張量負責擔任輸入窗口的角色，稱為Placeholder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3s760m5fzYg"
   },
   "outputs": [],
   "source": [
    "tensor = tf.placeholder(tf.float32, shape=(None, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVKws2zQfzYi"
   },
   "source": [
    "因為我們在訓練之前還尚未知道Data的數量，所以這裡使用None來表示未知。`tf.placeholder`在Graph階段是沒有數值的，必須等到Session階段才將數值給輸入進去。\n",
    "\n",
    "(4) 操作型張量：\n",
    "\n",
    "這類張量並不含有實際數值，而是一種操作，常用的「操作型張量」有兩種，第一種是作為最佳化使用，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUCAGyhkfzYi"
   },
   "outputs": [],
   "source": [
    "loss = ...\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvgxFoaPfzYj"
   },
   "source": [
    "選擇Optimizer和最佳化的方式來定義最佳化的操作方法，上述的例子是使用learning_rate為0.5的Gradient Descent來降低loss。\n",
    "\n",
    "另外一種是初始化的操作，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lclfbDwfzYk"
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5pAzf3NfzYl"
   },
   "source": [
    "這一個步驟是必要的但常常被忽略，還記得剛剛我們定義「變數張量」時有用到Initalizer，這些Initalizer在Graph完成時還不具有數值，必須使用`init_op`來給予數值，所以記住一定要放`init_op`進去Graph裡頭，而且必須先定義完成所有會用到的Initalizer再來設定這個`init_op`。\n",
    "\n",
    "### Session的操作\n",
    "\n",
    "「張量」元素具有兩個面向：功能和數值，在Graph階段「張量」只具有功能但不具有數值，只有到了Session階段才開始有數值，那如何將這些數值取出來呢？有兩種方法，以1+1當作範例來看看，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "bmwBI9VwfzYm",
    "outputId": "e2b8559f-8c89-4658-bb20-bed79674cadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    x = tf.constant(1)\n",
    "    y = tf.constant(1)\n",
    "    sol = tf.add(x,y) # add x and y\n",
    "\n",
    "with tf.Session(graph=g1) as sess: \n",
    "    print(sol) # print tensor, not their value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "miPCRXPxfzYo",
    "outputId": "9d05fbd2-2459-4c27-937f-9391c0445cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g1) as sess: \n",
    "    print(sol.eval()) # evaluate their value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "7nuqkaoOfzYp",
    "outputId": "bd58d823-3797-4cee-913b-18267d786256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "s1 = tf.Session(graph=g1)\n",
    "print(s1.run(sol)) # another way of evaluating value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTKAn9nafzYr"
   },
   "source": [
    "那如果我想使用placeholder來做到x+y呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "MKzo2tA-fzYr",
    "outputId": "91c99bf7-2afe-455c-fa2b-885b53e1e877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    x = tf.placeholder(tf.int32)\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    sol = tf.add(x,y) # add x and y\n",
    "\n",
    "s2 = tf.Session(graph=g2)\n",
    "\n",
    "# if x = 2 and y = 3\n",
    "print(s2.run(sol, feed_dict={x: 2, y: 3})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "Xm2uS2QCfzYu",
    "outputId": "d54d4029-b908-476c-d425-9483f3d22f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# if x = 5 and y = 7\n",
    "print(s2.run(sol, feed_dict={x: 5, y: 7})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zROE5xCUfzYw"
   },
   "source": [
    "因為x和y是placeholder，所以必須使用`feed_dict`來餵入相關資訊，否則會報錯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvY98ZFufzYw"
   },
   "source": [
    "### 第一個Tensorflow Model\n",
    "\n",
    "有了以上的認識我們就可以來建立我們第一個Model。\n",
    "\n",
    "以下我會使用物件導向的寫法，讓程式碼更有條理。\n",
    "\n",
    "Machine Learning在操作上可以整理成三個大步驟：建構(Building)、訓練(Fitting)和推論(Inference)，所以我們將會使用這三大步驟來建製我們的Model。\n",
    "\n",
    "在`SimpleLogisticClassification`裡頭，「建構」的動作在`__init__`中會進行，由`build`函式來建立Graph，其中我將Neurel Network的結構分離存於`structure`裡。「訓練」的動作在`fit`中進行，這裡採用傳統的Gradient Descent的方法，將所有Data全部考慮進去最佳化，未來會再介紹Batch Gradient Descent。最後，「推論」的部分在`predict`和`evaluate`中進行。\n",
    "\n",
    "`SimpleLogisticClassification`將會建構一個只有一層的Neurel Network，也就是說沒有Hidden Layer，畫個圖。\n",
    "\n",
    "![Simple Logistic Classification](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.002.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IkFZPA_LfzYw"
   },
   "outputs": [],
   "source": [
    "class SimpleLogisticClassification:\n",
    "\n",
    "    def __init__(self, n_features, n_labels, learning_rate=0.5):\n",
    "        self.n_features = n_features\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "        self.graph = tf.Graph()  # initialize new graph\n",
    "        self.build(learning_rate)  # building graph\n",
    "        self.sess = tf.Session(graph=self.graph)  # create session by the graph\n",
    "\n",
    "    def build(self, learning_rate):\n",
    "        # Building Graph\n",
    "        with self.graph.as_default():\n",
    "            ### Input\n",
    "            self.train_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "\n",
    "            ### Optimalization\n",
    "            # build neurel network structure and get their predictions and loss\n",
    "            self.y_, self.loss = self.structure(features=self.train_features,\n",
    "                                                         labels=self.train_labels)\n",
    "            # define training operation\n",
    "            self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "            ### Prediction\n",
    "            self.new_features = tf.placeholder(tf.float32, shape=(None, self.n_features))\n",
    "            self.new_labels = tf.placeholder(tf.int32, shape=(None, self.n_labels))\n",
    "            self.new_y_, self.new_loss = self.structure(features=self.new_features,\n",
    "                                                                       labels=self.new_labels)\n",
    "\n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def structure(self, features, labels):\n",
    "        # build neurel network structure and return their predictions and loss\n",
    "        ### Variable\n",
    "        if (not self.weights) or (not self.biases):\n",
    "            self.weights = {\n",
    "                'fc1': tf.Variable(tf.truncated_normal(shape=(self.n_features, self.n_labels))),\n",
    "            }\n",
    "            self.biases = {\n",
    "                'fc1': tf.Variable(tf.zeros(shape=(self.n_labels))),\n",
    "            }\n",
    "\n",
    "        ### Structure\n",
    "        # one fully connected layer\n",
    "        logits = self.getDenseLayer(features, self.weights['fc1'], self.biases['fc1'])\n",
    "\n",
    "        # predictions\n",
    "        y_ = tf.nn.softmax(logits)\n",
    "\n",
    "        # loss: softmax cross entropy\n",
    "        loss = tf.reduce_mean(\n",
    "                 tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "        return (y_, loss)\n",
    "\n",
    "    def getDenseLayer(self, input_layer, weight, bias, activation=None):\n",
    "        # fully connected layer\n",
    "        x = tf.add(tf.matmul(input_layer, weight), bias)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, X, y, epochs=10, validation_data=None, test_data=None):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "\n",
    "        self.sess.run(self.init_op)\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %2d/%2d: ' % (epoch+1, epochs))\n",
    "\n",
    "            # fully gradient descent\n",
    "            feed_dict = {self.train_features: X, self.train_labels: y}\n",
    "            self.sess.run(self.train_op, feed_dict=feed_dict)\n",
    "\n",
    "            # evaluate at the end of this epoch\n",
    "            y_ = self.predict(X)\n",
    "            train_loss = self.evaluate(X, y)\n",
    "            train_acc = self.accuracy(y_, y)\n",
    "            msg = ' loss = %8.4f, acc = %3.2f%%' % (train_loss, train_acc*100)\n",
    "\n",
    "            if validation_data:\n",
    "                val_loss = self.evaluate(validation_data[0], validation_data[1])\n",
    "                val_acc = self.accuracy(self.predict(validation_data[0]), validation_data[1])\n",
    "                msg += ', val_loss = %8.4f, val_acc = %3.2f%%' % (val_loss, val_acc*100)\n",
    "\n",
    "            print(msg)\n",
    "\n",
    "        if test_data:\n",
    "            test_acc = self.accuracy(self.predict(test_data[0]), test_data[1])\n",
    "            print('test_acc = %3.2f%%' % (test_acc*100))\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return (np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/predictions.shape[0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._check_array(X)\n",
    "        return self.sess.run(self.new_y_, feed_dict={self.new_features: X})\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "        return self.sess.run(self.new_loss, feed_dict={self.new_features: X, self.new_labels: y})\n",
    "\n",
    "    def _check_array(self, ndarray):\n",
    "        ndarray = np.array(ndarray)\n",
    "        if len(ndarray.shape) == 1:\n",
    "            ndarray = np.reshape(ndarray, (1, ndarray.shape[0]))\n",
    "        return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "dTVeEqwRfzYx",
    "outputId": "960805d5-00a6-4cec-d83b-ab0c785d243f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10: \n",
      " loss =   9.2515, acc = 12.81%, val_loss =   9.4888, val_acc = 11.92%\n",
      "Epoch  2/10: \n",
      " loss =   8.2946, acc = 13.89%, val_loss =   8.5156, val_acc = 13.10%\n",
      "Epoch  3/10: \n",
      " loss =   7.5609, acc = 15.92%, val_loss =   7.7680, val_acc = 15.02%\n",
      "Epoch  4/10: \n",
      " loss =   6.9563, acc = 18.31%, val_loss =   7.1521, val_acc = 17.44%\n",
      "Epoch  5/10: \n",
      " loss =   6.4402, acc = 20.94%, val_loss =   6.6249, val_acc = 19.80%\n",
      "Epoch  6/10: \n",
      " loss =   5.9915, acc = 23.35%, val_loss =   6.1650, val_acc = 22.38%\n",
      "Epoch  7/10: \n",
      " loss =   5.5971, acc = 25.79%, val_loss =   5.7596, val_acc = 24.98%\n",
      "Epoch  8/10: \n",
      " loss =   5.2479, acc = 28.18%, val_loss =   5.4001, val_acc = 27.30%\n",
      "Epoch  9/10: \n",
      " loss =   4.9376, acc = 30.46%, val_loss =   5.0803, val_acc = 29.86%\n",
      "Epoch 10/10: \n",
      " loss =   4.6608, acc = 32.71%, val_loss =   4.7947, val_acc = 32.20%\n",
      "test_acc = 33.58%\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLogisticClassification(n_features=28*28, n_labels=10, learning_rate= 0.5)\n",
    "model.fit(\n",
    "    X=train_data.images,\n",
    "    y=train_data.labels,\n",
    "    epochs=10,\n",
    "    validation_data=(valid_data.images, valid_data.labels),\n",
    "    test_data=(test_data.images, test_data.labels),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_Simple_Logistic_Classification_on_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
