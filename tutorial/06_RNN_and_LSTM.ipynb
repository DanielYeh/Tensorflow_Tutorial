{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tutorial 6: RNN and LSTM\n",
    "\n",
    "如果我們想要處理的問題是具有時序性的，該怎麼辦呢？本章將會介紹有時序性的Neurel Network。\n",
    "\n",
    "本單元程式碼LSTM部分可於[Github](https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/06_LSTM.py)下載。\n",
    "\n",
    "\n",
    "### 概論RNN\n",
    "\n",
    "當我們想使得Neurel Network具有時序性，我們的Neurel Network就必須有記憶的功能，然後在我不斷的輸入新資訊時，也能同時保有歷史資訊的影響，最簡單的作法就是將Output的結果保留，等到新資訊進來時，將新的資訊和舊的Output一起考量來訓練Neurel Network。\n",
    "\n",
    "![unrolling](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.010.jpeg)\n",
    "\n",
    "這種將舊有資訊保留的Neurel Network統稱為Recurrent Neural Networks (RNN)，這種不斷回饋的網路可以攤開來處理，如上圖，如果我有5筆數據，拿訓練一個RNN 5個回合並做了5次更新，其實就等效於攤開來一次處理5筆數據並做1次更新，這樣的手法叫做Unrolling，我們實作上會使用Unrolling的手法來增加計算效率。\n",
    "\n",
    "![RNN](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.011.jpeg)\n",
    "\n",
    "接下來來看RNN內部怎麼實現的，上圖是最簡單的RNN形式，我們將上一回產生的Output和這一回的Input一起評估出這一回的Output，詳細式子如下：\n",
    "\n",
    "new_o = tanh( i\\*W<sub>i</sub> + o\\*W<sub>o</sub> + B )\n",
    "\n",
    "如此一來RNN就具有時序性了，舊的歷史資料將可以被「記憶」起來，你可以把RNN的「記憶」看成是「短期記憶」，因為它只會記得上一回的Output而已。\n",
    "\n",
    "### 梯度消失與梯度爆炸\n",
    "\n",
    "但這種形式的RNN在實作上會遇到很大的問題，還記得第二章當中，我們有講過像是tanh這類有飽和區的函數，會造成梯度消失的問題，而我們如果使用Unrolling的觀點來看RNN，將會發現這是一個超級深的網路，Backpapagation必須一路通到t0的RNN，想當然爾，有些梯度將會消失，部分權重就更新不到了，那有一些聰明的讀者一定會想到，那就使用Relu就好啦！不過其實還有一個重要的因素造成梯度消失，同時也造成梯度爆炸。\n",
    "\n",
    "注意喔！雖然我們使用Unrolling的觀點，把網路看成是一個Deep網路的連接，但是和之前DNN不同之處，這些RNN彼此間是共享同一組權重的，這會造成梯度消失和梯度爆炸兩個問題，在RNN的結構裡頭，一個權重會隨著時間不斷的加強影響一個單一特徵，因為不同時間之下的RNN Cell共用同一個權重，這麼一來若是權重大於1，影響將會隨時間放大到梯度爆炸，若是權重小於1，影響將會隨時間縮小到梯度消失，就像是蝴蝶效應一般，微小的差異因為回饋的機制，而不合理的放大或是消失，因此RNN的Error Surface將會崎嶇不平，這會造成我們無法穩定的找到最佳解，難以收斂。這才是RNN難以使用的重要原因，把Activation Function換成Relu不會解決問題，文獻上反而告訴我們會變更差。\n",
    "\n",
    "解決梯度爆炸有一個聽起來很廢但廣為人們使用的方法，叫做Gradient Clipping，也就是只要在更新過程梯度超過一個值，我就切掉讓梯度維持在這個上限，這樣就不會爆炸啦，待會會講到的LSTM只能夠解決梯度消失問題，但不能解決梯度爆炸問題，因此我們還是需要Gradient Clipping方法的幫忙。\n",
    "\n",
    "在Tensorflow怎麼做到Gradient Clipping呢？作法是這樣的，以往我們使用`optimizer.minimize(loss)`來進行更新，事實上我們可以把這一步驟拆成兩部分，第一部分計算所有參數的梯度，第二部分使用這些梯度進行更新。因此我們可以從中作梗，把gradients偷天換日一番，一開始使用`optimizer.compute_gradients(loss)`來計算出個別的梯度，然後使用`tf.clip_by_global_norm(gradients, clip_norm)`來切梯度，最後再使用`optimizer.apply_gradients`把新的梯度餵入進行更新。\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM是現今RNN的主流，它可以解決梯度消失的問題，我們先來看看結構，先預告一下，LSTM是迄今為止這系列課程當中看過最複雜的Neurel Network。\n",
    "\n",
    "![LSTM](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.012.jpeg)\n",
    "\n",
    "最一開始和RNN一樣，Input會和上一回的Output一起評估一個「短期記憶」，\n",
    "\n",
    "f<sub>m</sub> = tanh( i\\*W<sub>mi</sub> + o\\*W<sub>mo</sub> + B<sub>m</sub> )\n",
    "\n",
    "但接下來不同於RNN直接輸出，LSTM做了一個類似於轉換成「長期記憶」的機制，「長期記憶」在這裡稱為State，State的狀態由三道門所控制，Input Gate負責控管哪些「短期記憶」可以進到「長期記憶」，Forget Gate負責調配哪一些「長期記憶」需要被遺忘，Output Gate則負責去決定需要從「長期記憶」中輸出怎樣的內容，先不要管這些Gate怎麼來，我們可以把這樣的記憶機制寫成以下的式子，假設State為f<sub>state</sub>、Input Gate為G<sub>i</sub>、Forget Gate為G<sub>f</sub>和Output Gate為G<sub>o</sub>。\n",
    "\n",
    "new_f<sub>state</sub> = G<sub>i</sub> \\* f<sub>m</sub> + G<sub>f</sub> \\* f<sub>state</sub>\n",
    "\n",
    "new_o = G<sub>o</sub>\\*tanh( new_f<sub>state</sub> )\n",
    "\n",
    "如果我們要使得上面中Gates的部分具有開關的功能的話，我們會希望Gates可以是0到1的值，0代表全關，1代表全開，sigmoid正可以幫我們做到這件事，那哪些因素會決定Gates的關閉與否呢？不妨考慮所有可能的因素，也就是所有輸入這個Cell的資訊都考慮進去，但上一回的State必須被剔除於外，因為上一回的State來決定下一個State的操作是不合理的，因此我們就可以寫下所有Gates的表示式了。\n",
    "\n",
    "G<sub>i</sub> = Sigmoid(i\\*W<sub>ii</sub> + o\\*W<sub>io</sub> + B<sub>i</sub>)\n",
    "\n",
    "G<sub>f</sub> = Sigmoid(i\\*W<sub>fi</sub> + o\\*W<sub>fo</sub> + B<sub>f</sub>)\n",
    "\n",
    "G<sub>o</sub> = Sigmoid(i\\*W<sub>oi</sub> + o\\*W<sub>oo</sub> + B<sub>o</sub>)\n",
    "\n",
    "這就是LSTM，「長期記憶」的出現可以解決掉梯度消失的問題，RNN只有「短期記憶」，所以一旦認為一個特徵不重要，經過幾回連乘，這個特徵的梯度就會消失殆盡，但是LSTM保留State，並且使用「加」的方法更新State，所以有一些重要的State得以留下來持續影響著Output，解決了梯度消失的問題。但是，不幸的LSTM還是免不了梯度爆炸，為什麼呢？如果一個特徵真的很重要，State會記住，Input也會強調，所以幾輪下來還是有可能出現爆炸的情況，這時候我們就需要Gradient Clipping的幫忙。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用LSTM實作文章產生器\n",
    "\n",
    "接下來我們來實作LSTM，目標是做一個文章產生器，我們希望機器可以不斷的根據前文猜測下一個「字母」(Letters)應該要下什麼，如此一來我只要給個開頭字母，LSTM就可以幫我腦補成一篇文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import zipfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading text8.zip\n",
      "Found and verified ./text8.zip\n",
      "=====\n",
      "Data size 100000000 letters\n",
      "=====\n",
      "Train Dataset: size: 99999000 letters,\n",
      "  first 64: ons anarchists advocate social relations based upon voluntary as\n",
      "Validation Dataset: size: 1000 letters,\n",
      "  first 64:  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "LETTER_SIZE = len(string.ascii_lowercase) + 1  # [a-z] + ' '\n",
    "FIRST_LETTER_ASCII = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def maybe_download(url, filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - FIRST_LETTER_ASCII + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + FIRST_LETTER_ASCII - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "    \n",
    "print('Downloading text8.zip')\n",
    "filename = maybe_download('http://mattmahoney.net/dc/text8.zip', './text8.zip', 31344016)\n",
    "\n",
    "print('=====')\n",
    "text = read_data(filename)\n",
    "print('Data size %d letters' % len(text))\n",
    "\n",
    "print('=====')\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('Train Dataset: size:', train_size, 'letters,\\n  first 64:', train_text[:64])\n",
    "print('Validation Dataset: size:', valid_size, 'letters,\\n  first 64:', valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面操作我們建制完成了字母庫，接下來就可以產生我們訓練所需要的Batch Data，所以我們來看看究竟要產生怎樣格式的資料。\n",
    "\n",
    "![LSTM Implement](https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.013.jpeg)\n",
    "\n",
    "如上圖所示，有點小複雜，假設我要設計一個LSTM Model，它的Unrolling Number為3，Batch Size為2，然後遇到的字串是\"abcde fghij klmno pqrst\"，接下來就開始產生每個Round要用的Data，產生的結果如上圖所示，你會發現產生的Data第0軸表示的是考慮unrolling需要取樣的資料，總共應該會有(Unrolling Number+1)筆，如上圖例，共有4筆，3筆當作輸入而3筆當作Labels，中間有2筆重疊使用，另外還有一點，我們會保留最後一筆Data當作下一個回合的第一筆，這是為了不浪費使用每一個字母前後的組合。而第1軸則是餵入單一LSTM需要的資料，我們一次可以餵多組不相干的字母進去，如上圖例，Batch Size=2所以餵2個字母進去，那這些不相干的字母在取樣的時候，我們會盡量讓它平均分配在文字庫，才能確保彼此之間不相干，以增加LSTM的訓練效率和效果。\n",
    "\n",
    "因此，先產生Batch Data吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** train_batches:\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "*** valid_batches:\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "def rnn_batch_generator(text, batch_size, num_unrollings):\n",
    "    text_size = len(text)\n",
    "\n",
    "    ### initialization\n",
    "    segment = text_size // batch_size\n",
    "    cursors = [offset * segment for offset in range(batch_size)]\n",
    "\n",
    "    batches = []\n",
    "    batch_initial = np.zeros(shape=(batch_size, LETTER_SIZE), dtype=np.float)\n",
    "    for i in range(batch_size):\n",
    "        cursor = cursors[i]\n",
    "        id_ = char2id(text[cursor])\n",
    "        batch_initial[i][id_] = 1.0\n",
    "\n",
    "        # move cursor\n",
    "        cursors[i] = (cursors[i] + 1) % text_size\n",
    "\n",
    "    batches.append(batch_initial)\n",
    "\n",
    "    ### generate loop\n",
    "    while True:\n",
    "        batches = [batches[-1], ]\n",
    "        for _ in range(num_unrollings):\n",
    "            batch = np.zeros(shape=(batch_size, LETTER_SIZE), dtype=np.float)\n",
    "            for i in range(batch_size):\n",
    "                cursor = cursors[i]\n",
    "                id_ = char2id(text[cursor])\n",
    "                batch[i][id_] = 1.0\n",
    "\n",
    "                # move cursor\n",
    "                cursors[i] = (cursors[i] + 1) % text_size\n",
    "            batches.append(batch)\n",
    "\n",
    "        yield batches  # [last batch of previous batches] + [unrollings]\n",
    "\n",
    "\n",
    "# demonstrate generator\n",
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "train_batches = rnn_batch_generator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = rnn_batch_generator(valid_text, 1, 1)\n",
    "\n",
    "print('*** train_batches:')\n",
    "print(batches2string(next(train_batches)))\n",
    "print(batches2string(next(train_batches)))\n",
    "print('*** valid_batches:')\n",
    "print(batches2string(next(valid_batches)))\n",
    "print(batches2string(next(valid_batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一下待會會用到的函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, LETTER_SIZE], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始建制LSTM Model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    def __init__(self, n_unrollings, n_memory, n_train_batch, learning_rate=1.0):\n",
    "        self.n_unrollings = n_unrollings\n",
    "        self.n_memory = n_memory\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.saved = None\n",
    "\n",
    "        self.graph = tf.Graph()  # initialize new grap\n",
    "        self.build(learning_rate, n_train_batch)  # building graph\n",
    "        self.sess = tf.Session(graph=self.graph)  # create session by the graph\n",
    "\n",
    "    def build(self, learning_rate, n_train_batch):\n",
    "        with self.graph.as_default():\n",
    "            ### Input\n",
    "            self.train_data = list()\n",
    "            for _ in range(self.n_unrollings + 1):\n",
    "                self.train_data.append(\n",
    "                    tf.placeholder(tf.float32, shape=[n_train_batch, LETTER_SIZE]))\n",
    "            self.train_inputs = self.train_data[:self.n_unrollings]\n",
    "            self.train_labels = self.train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "\n",
    "            ### Optimalization\n",
    "            # build neurel network structure and get their loss\n",
    "            self.y_, self.loss = self.structure(\n",
    "                inputs=self.train_inputs,\n",
    "                labels=self.train_labels,\n",
    "                n_batch=n_train_batch,\n",
    "            )\n",
    "\n",
    "            # define training operation\n",
    "\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "            # gradient clipping\n",
    "\n",
    "            # output gradients one by one\n",
    "            gradients, v = zip(*self.optimizer.compute_gradients(self.loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.25)  # clip gradient\n",
    "            # apply clipped gradients\n",
    "            self.train_op = self.optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "            ### Sampling and validation eval: batch 1, no unrolling.\n",
    "            self.sample_input = tf.placeholder(tf.float32, shape=[1, LETTER_SIZE])\n",
    "\n",
    "            saved_sample_output = tf.Variable(tf.zeros([1, self.n_memory]))\n",
    "            saved_sample_state = tf.Variable(tf.zeros([1, self.n_memory]))\n",
    "            self.reset_sample_state = tf.group(     # reset sample state operator\n",
    "                saved_sample_output.assign(tf.zeros([1, self.n_memory])),\n",
    "                saved_sample_state.assign(tf.zeros([1, self.n_memory])))\n",
    "\n",
    "            sample_output, sample_state = self.lstm_cell(\n",
    "                self.sample_input, saved_sample_output, saved_sample_state)\n",
    "            with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                          saved_sample_state.assign(sample_state)]):\n",
    "                # use tf.control_dependencies to make sure 'saving' before 'prediction'\n",
    "\n",
    "                self.sample_prediction = tf.nn.softmax(\n",
    "                    tf.nn.xw_plus_b(sample_output,\n",
    "                                    self.weights['classifier'],\n",
    "                                    self.biases['classifier']))\n",
    "\n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def lstm_cell(self, i, o, state):\n",
    "        \"\"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        ## Build Input Gate\n",
    "        ix = self.weights['input_gate_i']\n",
    "        im = self.weights['input_gate_o']\n",
    "        ib = self.biases['input_gate']\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        ## Build Forget Gate\n",
    "        fx = self.weights['forget_gate_i']\n",
    "        fm = self.weights['forget_gate_o']\n",
    "        fb = self.biases['forget_gate']\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        ## Memory\n",
    "        cx = self.weights['memory_i']\n",
    "        cm = self.weights['memory_o']\n",
    "        cb = self.biases['memory']\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        ## Update State\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        ## Build Output Gate\n",
    "        ox = self.weights['output_gate_i']\n",
    "        om = self.weights['output_gate_o']\n",
    "        ob = self.biases['output_gate']\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        ## Ouput\n",
    "        output = output_gate * tf.tanh(state)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    def structure(self, inputs, labels, n_batch):\n",
    "        ### Variable\n",
    "        if (not self.weights) or (not self.biases) or (not self.saved):\n",
    "            self.weights = {\n",
    "              'input_gate_i': tf.Variable(tf.truncated_normal(\n",
    "                  [LETTER_SIZE, self.n_memory], -0.1, 0.1)),\n",
    "              'input_gate_o': tf.Variable(tf.truncated_normal(\n",
    "                  [self.n_memory, self.n_memory], -0.1, 0.1)),\n",
    "              'forget_gate_i': tf.Variable(tf.truncated_normal(\n",
    "                  [LETTER_SIZE, self.n_memory], -0.1, 0.1)),\n",
    "              'forget_gate_o': tf.Variable(tf.truncated_normal(\n",
    "                  [self.n_memory, self.n_memory], -0.1, 0.1)),\n",
    "              'output_gate_i': tf.Variable(tf.truncated_normal(\n",
    "                  [LETTER_SIZE, self.n_memory], -0.1, 0.1)),\n",
    "              'output_gate_o': tf.Variable(tf.truncated_normal(\n",
    "                  [self.n_memory, self.n_memory], -0.1, 0.1)),\n",
    "              'memory_i': tf.Variable(tf.truncated_normal(\n",
    "                  [LETTER_SIZE, self.n_memory], -0.1, 0.1)),\n",
    "              'memory_o': tf.Variable(tf.truncated_normal(\n",
    "                  [self.n_memory, self.n_memory], -0.1, 0.1)),\n",
    "              'classifier': tf.Variable(tf.truncated_normal(\n",
    "                  [self.n_memory, LETTER_SIZE], -0.1, 0.1)),\n",
    "\n",
    "            }\n",
    "            self.biases = {\n",
    "              'input_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'forget_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'output_gate': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'memory': tf.Variable(tf.zeros([1, self.n_memory])),\n",
    "              'classifier': tf.Variable(tf.zeros([LETTER_SIZE])),\n",
    "            }\n",
    "\n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([n_batch, self.n_memory]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([n_batch, self.n_memory]), trainable=False)\n",
    "\n",
    "        ### Structure\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for input_ in inputs:\n",
    "            output, state = self.lstm_cell(input_, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([saved_output.assign(output),\n",
    "                                      saved_state.assign(state)]):\n",
    "            # use tf.control_dependencies to make sure 'saving' before 'calculating loss'\n",
    "\n",
    "            # Classifier\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(outputs, 0),\n",
    "                                     self.weights['classifier'],\n",
    "                                     self.biases['classifier'])\n",
    "            y_ = tf.nn.softmax(logits)\n",
    "            loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.concat(labels, 0), logits=logits))\n",
    "\n",
    "        return y_, loss\n",
    "\n",
    "    def initialize(self):\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "    def online_fit(self, X):\n",
    "        feed_dict = dict()\n",
    "        for i in range(self.n_unrollings + 1):\n",
    "            feed_dict[self.train_data[i]] = X[i]\n",
    "\n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def perplexity(self, X):\n",
    "        sum_logprob = 0\n",
    "        sample_size = len(X)-1\n",
    "        batch_size = X[0].shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            self.sess.run(self.reset_sample_state)\n",
    "            for j in range(sample_size):\n",
    "                sample_input = np.reshape(X[j][i], newshape=(1, -1))\n",
    "                sample_label = np.reshape(X[j+1][i], newshape=(1, -1))\n",
    "                predictions = self.sess.run(self.sample_prediction,\n",
    "                                            feed_dict={self.sample_input: sample_input})\n",
    "                sum_logprob += logprob(predictions, sample_label)\n",
    "        perplexity = float(np.exp(sum_logprob / batch_size / sample_size))\n",
    "        return perplexity\n",
    "\n",
    "    def generate(self, c, len_generate):\n",
    "        feed = np.array([[1 if id2char(i) == c else 0 for i in range(LETTER_SIZE)]])\n",
    "        sentence = characters(feed)[0]\n",
    "        self.sess.run(self.reset_sample_state)\n",
    "        for _ in range(len_generate):\n",
    "            prediction = self.sess.run(self.sample_prediction, feed_dict={self.sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 96s loss = 1.8350, perplexity = 6.0744\n",
      "Epoch 2/30: 93s loss = 1.5473, perplexity = 5.9950\n",
      "Epoch 3/30: 96s loss = 1.4832, perplexity = 5.7988\n",
      "Epoch 4/30: 95s loss = 1.4460, perplexity = 5.5873\n",
      "Epoch 5/30: 93s loss = 1.4268, perplexity = 6.0196\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.7728\n",
      "Generate From 'a':   a addressed trojp herregore efforts taxothers of fi\n",
      "Generate From 'h':   h a one nine one s personalt god tranant of genuali\n",
      "Generate From 'm':   m with the of retrintuutar one five zero and even t\n",
      "==========================================\n",
      "\n",
      "Epoch 6/30: 92s loss = 1.4116, perplexity = 5.8374\n",
      "Epoch 7/30: 92s loss = 1.3958, perplexity = 5.7529\n",
      "Epoch 8/30: 91s loss = 1.3911, perplexity = 5.8161\n",
      "Epoch 9/30: 92s loss = 1.3670, perplexity = 5.6386\n",
      "Epoch 10/30: 92s loss = 1.3871, perplexity = 5.5209\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.6448\n",
      "Generate From 'a':   as mark but use the church management seniorie othe\n",
      "Generate From 'h':   h mathum it layor j cape not pac feloghaokurg the a\n",
      "Generate From 'm':   ment condition christmishem the reasons obaging out\n",
      "==========================================\n",
      "\n",
      "Epoch 11/30: 92s loss = 1.3772, perplexity = 5.4907\n",
      "Epoch 12/30: 92s loss = 1.3782, perplexity = 6.1908\n",
      "Epoch 13/30: 92s loss = 1.3713, perplexity = 5.7394\n",
      "Epoch 14/30: 92s loss = 1.3722, perplexity = 6.5244\n",
      "Epoch 15/30: 92s loss = 1.3665, perplexity = 6.5655\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.6228\n",
      "Generate From 'a':   ans in the first glds for exclusively assistance es\n",
      "Generate From 'h':   h south and the w cops and goat right as known the \n",
      "Generate From 'm':   m charges has a properties keit was in second state\n",
      "==========================================\n",
      "\n",
      "Epoch 16/30: 362s loss = 1.3627, perplexity = 5.3342\n",
      "Epoch 17/30: 95s loss = 1.3674, perplexity = 5.2295\n",
      "Epoch 18/30: 93s loss = 1.3513, perplexity = 6.6203\n",
      "Epoch 19/30: 94s loss = 1.3637, perplexity = 5.9332\n",
      "Epoch 20/30: 94s loss = 1.3561, perplexity = 6.0590\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.4923\n",
      "Generate From 'a':   a the problems in mind types in one strieging call \n",
      "Generate From 'h':   huragre ray fundament lost knishera claokhen nalony\n",
      "Generate From 'm':   m for five one nine four zero market hell one nine \n",
      "==========================================\n",
      "\n",
      "Epoch 21/30: 93s loss = 1.3569, perplexity = 5.9601\n",
      "Epoch 22/30: 93s loss = 1.3516, perplexity = 6.9727\n",
      "Epoch 23/30: 92s loss = 1.3676, perplexity = 5.5722\n",
      "Epoch 24/30: 94s loss = 1.3603, perplexity = 6.1140\n",
      "Epoch 25/30: 92s loss = 1.3649, perplexity = 6.2638\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.5306\n",
      "Generate From 'a':   an experimenting meaning as dosil smold seven eight\n",
      "Generate From 'h':   h one nine seven biero shimm in died this theorothy\n",
      "Generate From 'm':   m to threat loss away a roon b one six four nine fa\n",
      "==========================================\n",
      "\n",
      "Epoch 26/30: 95s loss = 1.3533, perplexity = 6.1450\n",
      "Epoch 27/30: 75s loss = 1.3568, perplexity = 6.3603\n",
      "Epoch 28/30: 93s loss = 1.3719, perplexity = 5.4497\n",
      "Epoch 29/30: 96s loss = 1.3620, perplexity = 6.1687\n",
      "Epoch 30/30: 95s loss = 1.3660, perplexity = 5.9484\n",
      "\n",
      "=============== Validation ===============\n",
      "validation perplexity = 3.4477\n",
      "Generate From 'a':   ates in weaved to has be five six zero song in the \n",
      "Generate From 'h':   h a neil and would lockspry short there is attempte\n",
      "Generate From 'm':   man one nine zero eight moming between language yea\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build training batch generator\n",
    "batch_generator = rnn_batch_generator(\n",
    "    text=train_text,\n",
    "    batch_size=batch_size,\n",
    "    num_unrollings=num_unrollings,\n",
    ")\n",
    "\n",
    "# build validation data\n",
    "valid_batches = rnn_batch_generator(\n",
    "    text=valid_text, \n",
    "    batch_size=1, \n",
    "    num_unrollings=1,\n",
    ")\n",
    "\n",
    "valid_data = [np.array(next(valid_batches)) for _ in range(valid_size)]\n",
    "\n",
    "# build LSTM model\n",
    "model_LSTM = LSTM(\n",
    "    n_unrollings=num_unrollings,\n",
    "    n_memory=128,\n",
    "    n_train_batch=batch_size,\n",
    "    learning_rate=0.9\n",
    ")\n",
    "\n",
    "# initial model\n",
    "model_LSTM.initialize()\n",
    "\n",
    "# online training\n",
    "epochs = 30\n",
    "num_batchs_in_epoch = 5000\n",
    "valid_freq = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    avg_loss = 0\n",
    "    for _ in range(num_batchs_in_epoch):\n",
    "        batch = next(batch_generator)\n",
    "        loss = model_LSTM.online_fit(X=batch)\n",
    "        avg_loss += loss\n",
    "        \n",
    "    avg_loss = avg_loss / num_batchs_in_epoch\n",
    "    \n",
    "    train_perplexity = model_LSTM.perplexity(batch)\n",
    "    print('Epoch %d/%d: %ds loss = %6.4f, perplexity = %6.4f'\n",
    "           % ( epoch+1, epochs, time.time()-start_time, avg_loss, train_perplexity))\n",
    "    \n",
    "    if (epoch+1) % valid_freq == 0:\n",
    "        print('')\n",
    "        print('=============== Validation ===============')\n",
    "        print('validation perplexity = %6.4f' % (model_LSTM.perplexity(valid_data)))\n",
    "        print('Generate From 'a':  ', model_LSTM.generate(c='a', len_generate=50))\n",
    "        print('Generate From 'h':  ', model_LSTM.generate(c='h', len_generate=50))\n",
    "        print('Generate From 'm':  ', model_LSTM.generate(c='m', len_generate=50))\n",
    "        print('==========================================')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後來產生一篇以\"t\"為開頭的1000字文章吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tifician linulation fromsantinated inscriptions have been followne members of gomewhokeno science and direct to player by the xh music the work mercing a completely categories following were now shrries the graduate painters but three limil bp inversing to in show monasteria ziver buriale hollesthea or universities contains one nine five three villes on in wolf from home with alimon del wi tallation austry five he is generate three visitiral spectring greece of many proper six one would frequently to be along two zero zero one aberrieds him hockel alphaliatiss r kabif figant in jock final click hospite michael hetrion as the equations were feature to notably algebraic important but better can requires of the same since the many bag among the mastic five official with the homes abertosiar of game mi romannessas nine pp which based for a secrition in one nine five seven recent issannallies algorithm rigarborsphy inctmm information as provides an enjakitine on moll s bodies fit immeble one\n"
     ]
    }
   ],
   "source": [
    "print(model_LSTM.generate(c='t', len_generate=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看得出來LSTM想表達什麼嗎，哈哈！\n",
    "\n",
    "### Reference\n",
    "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
